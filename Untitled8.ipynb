{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNL1d5gRkXXCzTbAvjUG2Zv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yonatan-max/MyFirstRepository/blob/master/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeaFqrH72tJO",
        "colab_type": "code",
        "outputId": "aa3e6c0a-9cc6-4039-cf8b-7be3c7ae947b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#creating model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import optimizers\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "df=pd.read_table('text2notepad.txt',names=['English','Tigrigna'])\n",
        "tig_eng = array(df)\n",
        "tig_eng=tig_eng[:500:]\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(tig_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(tig_eng[:,0])\n",
        "# prepare tigrigna tokenizer\n",
        "tig_tokenizer = tokenization(tig_eng[:, 1])\n",
        "tig_vocab_size = len(tig_tokenizer.word_index) + 1\n",
        "tig_length =max_length(tig_eng[:,1])\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "# prepare training data\n",
        "trainX = encode_sequences(tig_tokenizer, tig_length, tig_eng[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, tig_eng[:, 0])\n",
        "# build NMT model\n",
        "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "      model.add(LSTM(units))\n",
        "      model.add(RepeatVector(out_timesteps))\n",
        "      model.add(LSTM(units, return_sequences=True))\n",
        "      model.add(Dense(out_vocab, activation='softmax'))\n",
        "      return model\n",
        "# model compilation\n",
        "model = define_model(tig_vocab_size, eng_vocab_size, tig_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
        "filename = 'tig_eng_model'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
        "                    verbose=1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 22 samples, validate on 6 samples\n",
            "Epoch 1/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 3.8917 - val_loss: 3.8332\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.83322, saving model to tig_eng_model\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 3.8193 - val_loss: 3.4212\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.83322 to 3.42124, saving model to tig_eng_model\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 3.3815 - val_loss: 3.8190\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 3.42124\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 4.9213 - val_loss: 3.5052\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 3.42124\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 3.3930 - val_loss: 3.2038\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.42124 to 3.20385, saving model to tig_eng_model\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 3.0836 - val_loss: 3.0334\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.20385 to 3.03344, saving model to tig_eng_model\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 2.9513 - val_loss: 3.0208\n",
            "\n",
            "Epoch 00007: val_loss improved from 3.03344 to 3.02084, saving model to tig_eng_model\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 2.8899 - val_loss: 2.9995\n",
            "\n",
            "Epoch 00008: val_loss improved from 3.02084 to 2.99950, saving model to tig_eng_model\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 2.8414 - val_loss: 3.0001\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.99950\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 2.7898 - val_loss: 2.9874\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.99950 to 2.98743, saving model to tig_eng_model\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 2.7264 - val_loss: 2.9861\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.98743 to 2.98605, saving model to tig_eng_model\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 2.6461 - val_loss: 2.9732\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.98605 to 2.97322, saving model to tig_eng_model\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 2.5481 - val_loss: 2.9740\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.97322\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 2.4399 - val_loss: 2.9684\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.97322 to 2.96839, saving model to tig_eng_model\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 2.3307 - val_loss: 3.0091\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.96839\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 2.2915 - val_loss: 3.3585\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 2.96839\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 2.5952 - val_loss: 2.8567\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.96839 to 2.85667, saving model to tig_eng_model\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 2.5504 - val_loss: 3.0673\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 2.85667\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 2.1149 - val_loss: 2.9687\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 2.85667\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.9895 - val_loss: 3.0276\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 2.85667\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.8920 - val_loss: 3.0060\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 2.85667\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.8363 - val_loss: 3.2802\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 2.85667\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.9584 - val_loss: 3.3157\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 2.85667\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 2.1528 - val_loss: 3.1787\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 2.85667\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.8992 - val_loss: 2.8760\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 2.85667\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.6047 - val_loss: 2.9865\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 2.85667\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.5558 - val_loss: 2.9062\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.85667\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.4763 - val_loss: 3.0479\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 2.85667\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.4256 - val_loss: 2.8861\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 2.85667\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.3460 - val_loss: 3.2530\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 2.85667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fa46f89cf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGTiuXgsEynn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train model\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "df=pd.read_table('text1notepad.txt',names=['English','Tigrigna'])\n",
        "tig_eng = array(df)\n",
        "#tig_eng=tig_eng[:500:]\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(tig_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(tig_eng[:,0])\n",
        "# prepare tigrigna tokenizer\n",
        "tig_tokenizer = tokenization(tig_eng[:, 1])\n",
        "tig_vocab_size = len(tig_tokenizer.word_index) + 1\n",
        "tig_length =max_length(tig_eng[:,1])\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "# prepare training data\n",
        "trainX = encode_sequences(tig_tokenizer, tig_length, tig_eng[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, tig_eng[:, 0])\n",
        "model = load_model('tig_eng_model')\n",
        "filename = 'tig_eng_model'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMBXY1-FzCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "c74040f8-96c2-4c97-f4e0-4faefbe7b5d2"
      },
      "source": [
        "#using model from english to tigrigna\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "df=pd.read_table('text3notepad.txt',names=['English'])\n",
        "eng = array(df)\n",
        "#teng=eng[:500:]\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "def max_length(lines):\n",
        "\treturn max(len(line) for line in lines)\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(eng[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length =max_length(eng)\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "testX= encode_sequences(eng_tokenizer, eng_length,eng[:,0])\n",
        "model = load_model('tig_eng_model')\n",
        "\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        " \n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        " \n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tpredicted = list()\n",
        "\tfor  source in (sources):\n",
        "\t\t # translate encoded source text\n",
        "\t  \tsource = source.reshape((1, source.shape[0]))\n",
        "\t  \ttranslation = predict_sequence(model,tokenizer, source)\n",
        "\t   \tpredicted.append(translation.split())\n",
        "  return predicted\n",
        "evaluate_model(model, eng_tokenizer, testX, eng[:,0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-a4bc31b080fa>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    predicted.append(translation.split())\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEix2_jDpGcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}