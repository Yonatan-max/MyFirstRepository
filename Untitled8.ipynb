{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnQXO//M5pJmt7hvPuLWmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yonatan-max/MyFirstRepository/blob/master/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeaFqrH72tJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import optimizers\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "df=pd.read_table('text2notepad.txt',names=['English','Tigrigna'])\n",
        "tig_eng = array(df)\n",
        "tig_eng=tig_eng[:500:]\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(tig_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(tig_eng[:,0])\n",
        "# prepare tigrigna tokenizer\n",
        "tig_tokenizer = tokenization(tig_eng[:, 1])\n",
        "tig_vocab_size = len(tig_tokenizer.word_index) + 1\n",
        "tig_length =max_length(tig_eng[:,1])\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "# prepare training data\n",
        "trainX = encode_sequences(tig_tokenizer, tig_length, tig_eng[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, tig_eng[:, 0])\n",
        "# build NMT model\n",
        "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "      model.add(LSTM(units))\n",
        "      model.add(RepeatVector(out_timesteps))\n",
        "      model.add(LSTM(units, return_sequences=True))\n",
        "      model.add(Dense(out_vocab, activation='softmax'))\n",
        "      return model\n",
        "# model compilation\n",
        "model = define_model(tig_vocab_size, eng_vocab_size, tig_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
        "filename = 'tig_eng_model'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGTiuXgsEynn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train model\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "df=pd.read_table('text1notepad.txt',names=['English','Tigrigna'])\n",
        "tig_eng = array(df)\n",
        "#tig_eng=tig_eng[:500:]\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(tig_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(tig_eng[:,0])\n",
        "# prepare tigrigna tokenizer\n",
        "tig_tokenizer = tokenization(tig_eng[:, 1])\n",
        "tig_vocab_size = len(tig_tokenizer.word_index) + 1\n",
        "tig_length =max_length(tig_eng[:,1])\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "# prepare training data\n",
        "trainX = encode_sequences(tig_tokenizer, tig_length, tig_eng[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, tig_eng[:, 0])\n",
        "model = load_model('tig_eng_model')\n",
        "filename = 'tig_eng_model'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMBXY1-FzCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "c74040f8-96c2-4c97-f4e0-4faefbe7b5d2"
      },
      "source": [
        "#using model from english to tigrigna\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "df=pd.read_table('text3notepad.txt',names=['English'])\n",
        "eng = array(df)\n",
        "#teng=eng[:500:]\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "def max_length(lines):\n",
        "\treturn max(len(line) for line in lines)\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(eng[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length =max_length(eng)\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "testX= encode_sequences(eng_tokenizer, eng_length,eng[:,0])\n",
        "model = load_model('tig_eng_model')\n",
        "\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        " \n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        " \n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tpredicted = list()\n",
        "\tfor  source in (sources):\n",
        "\t\t # translate encoded source text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "\t  \tsource = source.reshape((1, source.shape[0]))\n",
        "\t  \ttranslation = predict_sequence(model,tokenizer, source)\n",
        "\t   \tpredicted.append(translation.split())\n",
        "  return predicted\n",
        "evaluate_model(model, eng_tokenizer, testX, eng[:,0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-a4bc31b080fa>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    predicted.append(translation.split())\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEix2_jDpGcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}