{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfutBtvSNnMY3RJsOVdGtC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yonatan-max/MyFirstRepository/blob/master/Tig_Eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeaFqrH72tJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import optimizers\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "df=pd.read_table('text2notepad.txt',names=['English','Tigrigna'])\n",
        "tig_eng = array(df)\n",
        "tig_eng=tig_eng[:500:]\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(tig_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(tig_eng[:,0])\n",
        "# prepare tigrigna tokenizer\n",
        "tig_tokenizer = tokenization(tig_eng[:, 1])\n",
        "tig_vocab_size = len(tig_tokenizer.word_index) + 1\n",
        "tig_length =max_length(tig_eng[:,1])\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "# prepare training data\n",
        "trainX = encode_sequences(tig_tokenizer, tig_length, tig_eng[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, tig_eng[:, 0])\n",
        "# build NMT model\n",
        "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "      model.add(LSTM(units))\n",
        "      model.add(RepeatVector(out_timesteps))\n",
        "      model.add(LSTM(units, return_sequences=True))\n",
        "      model.add(Dense(out_vocab, activation='softmax'))\n",
        "      return model\n",
        "# model compilation\n",
        "model = define_model(tig_vocab_size, eng_vocab_size, tig_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
        "filename = 'tig_eng_model'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGTiuXgsEynn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train model\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "df=pd.read_table('text1notepad.txt',names=['English','Tigrigna'])\n",
        "tig_eng = array(df)\n",
        "#tig_eng=tig_eng[:500:]\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(tig_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(tig_eng[:,0])\n",
        "# prepare tigrigna tokenizer\n",
        "tig_tokenizer = tokenization(tig_eng[:, 1])\n",
        "tig_vocab_size = len(tig_tokenizer.word_index) + 1\n",
        "tig_length =max_length(tig_eng[:,1])\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "# prepare training data\n",
        "trainX = encode_sequences(tig_tokenizer, tig_length, tig_eng[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, tig_eng[:, 0])\n",
        "model = load_model('tig_eng_model')\n",
        "filename = 'tig_eng_model'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMBXY1-FzCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using model from english to tigrigna\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array,argmax\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# function to build a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "def max_length(lines):\n",
        "\treturn max(len(line) for line in lines)\n",
        " \n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "\n",
        "df=pd.read_table('text3notepad.txt',names=['English','Tigrigna'])\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(df['English'].tolist())\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(df['English'].tolist())\n",
        "# prepare data\n",
        "trainX = encode_sequences(eng_tokenizer,eng_length,df['English'].tolist())\n",
        "testX = encode_sequences(eng_tokenizer,eng_length,df['English'].tolist())\n",
        "# load model\n",
        "model = load_model('tig_eng_model')\n",
        "testX=testX.reshape((testX.shape[0],testX.shape[1]))\n",
        "source=[i[0:4] for i in testX]\n",
        "source=array(source)\n",
        "preds = model.predict_classes(source)\n",
        "def get_word(n, tokenizer):\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == n:\n",
        "              return word\n",
        "      return None\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "       temp = []\n",
        "       for j in range(len(i)):\n",
        "            t = get_word(i[j],tig_tokenizer)\n",
        "            if j > 0:\n",
        "                if (t == get_word(i[j-1],tig_tokenizer)) or (t == None):\n",
        "                     temp.append('')\n",
        "                else:\n",
        "                     temp.append(t)\n",
        "            else:\n",
        "                   if(t == None):\n",
        "                          temp.append('')\n",
        "                   else:\n",
        "                          temp.append(t) \n",
        "\n",
        "       preds_text.append(' '.join(temp))\n",
        "pred_df =pd.DataFrame({'TobePred':df['English'],'actual' :df['Tigrigna'], 'predicted' : preds_text})\n",
        "pred_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjWGU4TinaUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using model from tigrigna to english\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array,argmax\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# function to build a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "def max_length(lines):\n",
        "\treturn max(len(line) for line in lines)\n",
        " \n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "\n",
        "df=pd.read_table('text3notepad.txt',names=['English','Tigrigna'])\n",
        "# prepare tigrigna tokenizer\n",
        "tig_tokenizer = create_tokenizer(df['Tigrigna'].tolist())\n",
        "tig_vocab_size = len(tig_tokenizer.word_index) + 1\n",
        "tig_length = max_length(df['Tigrigna'].tolist())\n",
        "# prepare data\n",
        "trainX = encode_sequences(tig_tokenizer,tig_length,df['Tigrigna'].tolist())\n",
        "testX = encode_sequences(tig_tokenizer,tig_length,df['Tigrigna'].tolist())\n",
        "# load model\n",
        "model = load_model('tig_eng_model')\n",
        "testX=testX.reshape((testX.shape[0],testX.shape[1]))\n",
        "source=[i[0:4] for i in testX]\n",
        "source=array(source)\n",
        "preds = model.predict_classes(source)\n",
        "def get_word(n, tokenizer):\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == n:\n",
        "              return word\n",
        "      return None\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "       temp = []\n",
        "       for j in range(len(i)):\n",
        "            t = get_word(i[j],eng_tokenizer)\n",
        "            if j > 0:\n",
        "                if (t == get_word(i[j-1],eng_tokenizer)) or (t == None):\n",
        "                     temp.append('')\n",
        "                else:\n",
        "                     temp.append(t)\n",
        "            else:\n",
        "                   if(t == None):\n",
        "                          temp.append('')\n",
        "                   else:\n",
        "                          temp.append(t) \n",
        "\n",
        "       preds_text.append(' '.join(temp))\n",
        "pred_df = pd.DataFrame({'TobePred':df['Tigrigna'],'actual' :df['English'], 'predicted' : preds_text})\n",
        "pred_df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}